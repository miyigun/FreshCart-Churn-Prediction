{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a756b2",
   "metadata": {},
   "source": [
    "<h2>06. Nihai Üretim Hattı (Final Production Pipeline)</h2>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>Proje Detayları</h3>\n",
    "<ul>\n",
    "    <li><b>Proje:</b> FreshCart Müşteri Kaybı Tahmini (Customer Churn Prediction)</li>\n",
    "    <li><b>Amaç:</b> Uçtan Uca Veri İşleme ve Model Eğitimi Hattı (End-to-End Data Processing & Model Training Pipeline)</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>Amaç</h3>\n",
    "<p>\n",
    "    Bu betik, önceki tüm adımları (Veri Yükleme, Özellik Mühendisliği ve Modelleme) tek, tekrarlanabilir bir <b>üretim hattında</b> (pipeline) birleştirir. Bir üretim eğitimi çalıştırmasını simüle eder:\n",
    "</p>\n",
    "\n",
    "<ol>\n",
    "    <li><b>Ham Veriyi Yükle</b> (Load Raw Data)</li>\n",
    "    <li><b>Kesme Stratejisi Uygula</b> (Sızıntıyı Önle) (Apply Cutoff Strategy - Prevent Leakage)</li>\n",
    "    <li><b>Tüm Özellikleri Oluştur</b> (RFM + Davranışsal + Gelişmiş) (Generate All Features)</li>\n",
    "    <li><b>Nihai Modeli Eğit</b> (Optimize Edilmiş Hiperparametreleri Kullanarak) (Train Final Model)</li>\n",
    "    <li><b>Dağıtım İçin Yapıtları Dışa Aktar</b> (Model ve Meta Veriler) (Export Artifacts)</li>\n",
    "</ol>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55f275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kütüphaneler başarıyla yüklendi.\n"
     ]
    }
   ],
   "source": [
    "#Kütüphaneleri import etme\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, precision_recall_curve\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Kütüphaneler başarıyla yüklendi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc1efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Özel modülleri içe aktarmak için 'src' dizinini yola ekle\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3741bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import RAW_DATA_DIR, PROCESSED_DATA_DIR, MODEL_DIR, RANDOM_STATE\n",
    "from data.data_loader import InstacartDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd216368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortam Kurulumu Tamamlandı\n"
     ]
    }
   ],
   "source": [
    "# Özellik Mühendisliği Modüllerini İçe Aktar\n",
    "from features.rfm_features import RFMFeatureEngineer\n",
    "from features.behavioral_features import BehavioralFeatureEngineer\n",
    "\n",
    "print(\"Ortam Kurulumu Tamamlandı\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a346f2",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "Adım 1: Ham Veriyi Al\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a431c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(self):\n",
    "        \"\"\"Ham Instacart veri kümelerini yükler ve önceki/eğitim siparişlerini birleştirir.\"\"\"\n",
    "                \n",
    "        print(\"Ham Veri Yükleniyor...\")\n",
    "        \n",
    "        # config.py dosyasındaki yolu kullanarak veri yükleyiciyi başlat\n",
    "        loader = InstacartDataLoader(RAW_DATA_DIR)\n",
    "        \n",
    "        # Tüm veri kümelerini bir sözlüğe (dictionary) yükle\n",
    "        data = loader.load_all_data()\n",
    "\n",
    "        # Ana veri çerçevelerini (dataframes) ayır\n",
    "        orders_df = data['orders']\n",
    "        products_df = data['products']\n",
    "        \n",
    "        # Önceki (prior) ve eğitim (train) sipariş ürünlerini tek bir veri çerçevesinde birleştir (concatenate)\n",
    "        order_products = pd.concat([\n",
    "            data['order_products_prior'],\n",
    "            data['order_products_train']\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        print(f\"Veri Yüklendi. Siparişler: {len(orders_df):,}, Ürünler: {len(products_df):,}\")\n",
    "        \n",
    "        # Üretim hattı (pipeline) için gerekli tüm veri çerçevelerini döndür\n",
    "        return orders_df, products_df, order_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34cfd7",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "Adım 2: Özellik Mühendisliği Hattı (\"Sızıntısız\" Mantık)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0c8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trend(series):\n",
    "    \"\"\"\n",
    "    Hesaplama maliyetinden tasarruf etmek için sadece en az 2 veri noktasına sahip seriler için eğimi (slope) hesaplar.\n",
    "    \"\"\"\n",
    "    if len(series) < 2:\n",
    "        return 0\n",
    "    try:\n",
    "        # x zamandır (sipariş sırası), y değerdir (sepet büyüklüğü vb.)\n",
    "        # stats.linregress, doğrusal regresyon parametrelerini döndürür.\n",
    "        slope, _, _, _, _ = stats.linregress(np.arange(len(series)), series.values)\n",
    "        return slope\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5e3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_pipeline(orders_df, order_products, products_df):\n",
    "    \"\"\"\n",
    "    Tüm özellik mühendisliği hattını 03_feature_engineering.ipynb mantığına uygun olarak çalıştırır.\n",
    "    \"\"\"\n",
    "    print(\"\\nÖzellik Üretim Hattı Başlatılıyor...\")\n",
    "    \n",
    "    # 1. SIRALAMA VE BÖLME (Kesme Stratejisi)\n",
    "    print(\"1. Kesme Stratejisi Uygulanıyor (Geçmiş ve Gelecek Ayrımı Yapılıyor)...\")\n",
    "    orders_sorted = orders_df.sort_values(['user_id', 'order_number'])\n",
    "    last_orders = orders_sorted.groupby('user_id').tail(1)  # Hedef (Target)\n",
    "    orders_history = orders_sorted.drop(last_orders.index)  # Özellikler Geçmişi (Features History)\n",
    "    \n",
    "    # order_products verilerini sadece geçmiş siparişler için filtrele\n",
    "    op_history = order_products[order_products['order_id'].isin(orders_history['order_id'])]\n",
    "    \n",
    "    # 2. HEDEFLERİ OLUŞTUR\n",
    "    print(\"2. Hedefler Oluşturuluyor...\")\n",
    "    labels = last_orders[['user_id', 'days_since_prior_order']].copy()\n",
    "    # Eğer son siparişten bu yana geçen gün sayısı >= 30 ise 'churn' (kayıp) olarak etiketle\n",
    "    labels['is_churn'] = (labels['days_since_prior_order'] >= 30).astype(int)\n",
    "    \n",
    "    # 3. RFM ÖZELLİKLERİ\n",
    "    print(\"3. RFM Özellikleri Oluşturuluyor (Sadece Ham Metrikler)...\")\n",
    "    rfm_eng = RFMFeatureEngineer()\n",
    "    rfm_feats = rfm_eng.create_all_rfm_features(orders_history, op_history)\n",
    "        \n",
    "    # Özel Risk ve Değer Metrikleri\n",
    "    rfm_feats['clv_proxy'] = rfm_feats['total_orders'] * rfm_feats['avg_basket_size']\n",
    "    rfm_feats['engagement_score'] = rfm_feats['orders_per_day'] * rfm_feats['total_items_ordered']\n",
    "    rfm_feats['at_risk_score'] = rfm_feats['days_since_last_order'] / (rfm_feats['avg_days_between_orders'] + 1)\n",
    "    \n",
    "    # 4. DAVRANIŞSAL ÖZELLİKLER\n",
    "    print(\"4. Davranışsal Özellikler Oluşturuluyor...\")\n",
    "    beh_eng = BehavioralFeatureEngineer()\n",
    "    beh_feats = beh_eng.create_all_behavioral_features(orders_history, op_history, products_df)\n",
    "    \n",
    "    # 5. ZAMAN SERİSİ / EĞİLİM (TREND) ÖZELLİKLERİ\n",
    "    print(\"5. Zaman Serisi Eğilimleri Türetiliyor...\")\n",
    "    \n",
    "    # Veriyi kullanıcıya göre grupla\n",
    "    # Sadece hesaplama için gerekli sütunları al\n",
    "    user_trends = orders_history.groupby('user_id').agg({\n",
    "        'days_since_prior_order': list  # Her kullanıcı için siparişler arasındaki günlerin listesi\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Sepet büyüklüğü eğilimi için order_products, orders ile birleşmelidir\n",
    "    order_sizes = op_history.groupby('order_id').size().reset_index(name='basket_size')\n",
    "    order_sizes = order_sizes.merge(orders_history[['order_id', 'user_id']], on='order_id')\n",
    "    basket_trends = order_sizes.groupby('user_id').agg({'basket_size': list}).reset_index()\n",
    "    \n",
    "    # Eğilim Fonksiyonlarını Uygula\n",
    "    user_trends['order_frequency_trend'] = user_trends['days_since_prior_order'].apply(lambda x: calculate_trend(pd.Series(x).dropna()))\n",
    "    basket_trends['basket_size_trend'] = basket_trends['basket_size'].apply(lambda x: calculate_trend(pd.Series(x)))\n",
    "    \n",
    "    # 6. ORAN (RATIO) ÖZELLİKLERİ\n",
    "    print(\"6. Hız (Velocity) ve İvme (Acceleration) Metrikleri Oluşturuluyor...\")\n",
    "    # Hız (Velocity)\n",
    "    rfm_feats['purchase_velocity'] = 1 / (rfm_feats['avg_days_between_orders'] + 1)\n",
    "    \n",
    "    # İvme (Acceleration) (Son sipariş / Ortalama siparişler arası günler)\n",
    "    # > 1 yavaşlama demektir (Kaybetme riski), < 1 hızlanma demektir\n",
    "    rfm_feats['recency_acceleration'] = rfm_feats['days_since_last_order'] / (rfm_feats['avg_days_between_orders'] + 0.01)\n",
    "    \n",
    "    # Etkileşim (Interaction)\n",
    "    rfm_feats['recency_x_frequency'] = rfm_feats['days_since_last_order'] * rfm_feats['total_orders']\n",
    "\n",
    "    # 7. HEPSİNİ BİRLEŞTİR (MERGE)\n",
    "    print(\"7. Özellik Kümeleri Birleştiriliyor...\")\n",
    "    final_df = labels[['user_id', 'is_churn']].merge(rfm_feats, on='user_id', how='left')\n",
    "    final_df = final_df.merge(beh_feats, on='user_id', how='left')\n",
    "    final_df = final_df.merge(user_trends[['user_id', 'order_frequency_trend']], on='user_id', how='left')\n",
    "    final_df = final_df.merge(basket_trends[['user_id', 'basket_size_trend']], on='user_id', how='left')\n",
    "    \n",
    "    # NaN değerleri 0 ile doldur\n",
    "    final_df = final_df.fillna(0)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5411adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim (TRAINING) verisinden bölmeleri (bins) almak için yardımcı fonksiyon\n",
    "def get_qcut_bins(data_series, q):\n",
    "    \"\"\"YALNIZCA eğitim veri serisinden kantil bölmelerini (quantile bins) hesaplar.\"\"\"\n",
    "    # retbins=True bölme kenarlarını (bin edges) döndürür\n",
    "    return pd.qcut(data_series, q=q, retbins=True, duplicates='drop')[1]\n",
    "\n",
    "# Bölmeleri TEST verisine uygulamak için yardımcı fonksiyon\n",
    "def apply_qcut_bins(test_series, bins, labels):\n",
    "    \"\"\"Önceden hesaplanmış bölmeleri eğitim verisinden test serisine uygular.\"\"\"\n",
    "    # pd.cut, eğitim kümesinden hesaplanan açık bölmeleri uygulamak için kullanılır.\n",
    "    # right=False aralığın [a, b) olmasını sağlar, bu da veri sızıntısını (leakage) önler.\n",
    "    return pd.cut(\n",
    "        test_series, \n",
    "        bins=bins, \n",
    "        labels=labels, \n",
    "        include_lowest=True, \n",
    "        right=False  # Tutarlılık ve sızıntıyı önlemek için kritik\n",
    "    ).astype(float).fillna(0).astype(int) # Test verisi eğitim bölmelerinin dışına düşerse dayanıklılık için fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141bcd7",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "Adım 3: Nihai Model Eğitimi (Final Model Training)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca4ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(final_dataset):\n",
    "    print(\"\\nEğitim İçin Veri Hazırlanıyor...\")\n",
    "\n",
    "    # 1. X ve y'yi Hazırla\n",
    "    # 'user_id' ve 'is_churn' sütunlarını özelliklerden kaldır\n",
    "    feature_cols = [c for c in final_dataset.columns if c not in ['user_id', 'is_churn']]\n",
    "    \n",
    "    X = final_dataset[feature_cols].copy()\n",
    "    y = final_dataset['is_churn']\n",
    "\n",
    "    # 2. Eğitim/Doğrulama İçin Bölme (Split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    print(f\"Veri bölme tamamlandı. X_train boyutu: {X_train.shape}, X_test boyutu: {X_test.shape}\")\n",
    "\n",
    "    # 3. En İyi Hiperparametreleri Yükle\n",
    "    try:\n",
    "        with open(MODEL_DIR / 'best_params.json', 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "        print(\"En İyi Hiperparametreler önceki adımdan yüklendi.\")\n",
    "    except Exception as e:\n",
    "        print(f\"En iyi parametreler dosyası bulunamadı. Varsayılan parametreler kullanılıyor. Hata: {e}\")\n",
    "        best_params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': 0.05,\n",
    "            'n_estimators': 1000,\n",
    "            'scale_pos_weight': 2.3\n",
    "        }\n",
    "\n",
    "    # 4. LightGBM'i Eğit\n",
    "    print(\"\\nNihai LightGBM Modeli Eğitiliyor...\")\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
    "\n",
    "    final_model = lgb.train(\n",
    "        best_params,\n",
    "        dtrain,\n",
    "        valid_sets=[dvalid],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    # --- OPTİMAL EŞİK DEĞERİ (THRESHOLD) HESAPLAMASI ---\n",
    "    print(\"\\nOptimal Eşik Değeri Hesaplanıyor...\")\n",
    "    \n",
    "    y_pred_proba = final_model.predict(X_test)\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    # F1 skorunu hesapla\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    \n",
    "    print(f\"Bulunan En İyi Eşik Değeri: {best_threshold:.4f}\")\n",
    "    \n",
    "    # Eşik değerinin modelle eşleşmesini sağlamak için hemen kaydet\n",
    "    threshold_path = MODEL_DIR / 'optimal_threshold.json'\n",
    "    with open(threshold_path, 'w') as f:\n",
    "        json.dump({'threshold': float(best_threshold)}, f)\n",
    "    print(f\"Eşik Değeri şuraya kaydedildi: {threshold_path}\")\n",
    "\n",
    "    print(\"Model Eğitimi Tamamlandı.\")\n",
    "    \n",
    "    # feature_cols listesini ham haliyle döndürüyoruz (Skorlama yok)\n",
    "    return final_model, X_test, y_test, feature_cols, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729a98e",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "Adım 4: Hızlı Doğrulama ve Sağlama Kontrolü (Quick Validation & Sanity Check)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176015a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, X_test, y_test, threshold):\n",
    "    print(\"\\nModel Performansı Doğrulanıyor...\")\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    \n",
    "    # Dinamik eşik değeri (threshold) kullanıyoruz\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "    # AUC (Eğri Altındaki Alan) skorunu hesapla\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    # F1 skorunu hesapla\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Nihai AUC Skoru: {auc:.4f}\")\n",
    "    print(f\"Nihai F1 Skoru : {f1:.4f} (eşik değerinde {threshold:.4f})\")\n",
    "    \n",
    "    print(\"\\nSınıflandırma Raporu (Classification Report):\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec16c08",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "Adım 5: Dağıtım İçin Ürünleri Dışa Aktar (Export Artifacts for Deployment)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc2e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artifacts(model, feature_cols, final_dataset):\n",
    "    print(\"\\nÜretim Yapıtları Dışa Aktarılıyor...\")\n",
    "\n",
    "    # 1. Modeli Kaydet\n",
    "    model_path = MODEL_DIR / 'final_model_optimized.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\" -> Model şuraya kaydedildi: {model_path}\")\n",
    "\n",
    "    # 2. Özellik Listesini Kaydet (API girdi doğrulaması için kritiktir)\n",
    "    # PROCESSED dizinine kaydet (Veri Hattı Standardı)\n",
    "    feature_path_processed = PROCESSED_DATA_DIR / 'model_features.json'\n",
    "    with open(feature_path_processed, 'w') as f:\n",
    "        json.dump(feature_cols, f)\n",
    "    print(f\" -> Özellik listesi şuraya kaydedildi: {feature_path_processed}\")\n",
    "\n",
    "    # MODELS dizinine kaydet (App.py Standardı - KEYERROR hatası için düzeltme)\n",
    "    feature_path_models = MODEL_DIR / 'feature_names.json'\n",
    "    with open(feature_path_models, 'w') as f:\n",
    "        json.dump(feature_cols, f)\n",
    "    print(f\" -> Özellik listesi şuraya senkronize edildi: {feature_path_models}\")\n",
    "\n",
    "    # 3. Veri Kümesini Kaydet (İsteğe Bağlı, Dashboard EDA için)\n",
    "    data_path = PROCESSED_DATA_DIR / 'final_features_advanced.parquet'\n",
    "    final_dataset.to_parquet(data_path)\n",
    "    print(f\" -> İşlenmiş veri şuraya kaydedildi: {data_path}\")\n",
    "\n",
    "    print(\"\\nÜRETİM HATTI BAŞARIYLA TAMAMLANDI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f00a9",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "ANA ÇALIŞTIRMA (MAIN EXECUTION)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d49cf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data.data_loader:Instacart veri setleri yükleniyor...\n",
      "INFO:data.data_loader:Yükleniyor: orders.csv...\n",
      "INFO:data.data_loader:Yüklendi orders: (3421083, 7)\n",
      "INFO:data.data_loader:Yükleniyor: order_products__prior.csv...\n",
      "INFO:data.data_loader:Yüklendi order_products_prior: (32434489, 4)\n",
      "INFO:data.data_loader:Yükleniyor: order_products__train.csv...\n",
      "INFO:data.data_loader:Yüklendi order_products_train: (1384617, 4)\n",
      "INFO:data.data_loader:Yükleniyor: products.csv...\n",
      "INFO:data.data_loader:Yüklendi products: (49688, 4)\n",
      "INFO:data.data_loader:Yükleniyor: aisles.csv...\n",
      "INFO:data.data_loader:Yüklendi aisles: (134, 2)\n",
      "INFO:data.data_loader:Yükleniyor: departments.csv...\n",
      "INFO:data.data_loader:Yüklendi departments: (21, 2)\n",
      "INFO:data.data_loader:Tüm veri setleri başarıyla yüklendi!\n",
      "\n",
      "INFO:data.data_loader:================================================================================\n",
      "INFO:data.data_loader:VERİ ÖZETİ\n",
      "INFO:data.data_loader:================================================================================\n",
      "INFO:data.data_loader:orders                   :  3,421,083 satır x   7 sütun\n",
      "INFO:data.data_loader:                           Bellek: 358.81 MB\n",
      "INFO:data.data_loader:order_products_prior     : 32,434,489 satır x   4 sütun\n",
      "INFO:data.data_loader:                           Bellek: 989.82 MB\n",
      "INFO:data.data_loader:order_products_train     :  1,384,617 satır x   4 sütun\n",
      "INFO:data.data_loader:                           Bellek: 42.26 MB\n",
      "INFO:data.data_loader:products                 :     49,688 satır x   4 sütun\n",
      "INFO:data.data_loader:                           Bellek: 5.31 MB\n",
      "INFO:data.data_loader:aisles                   :        134 satır x   2 sütun\n",
      "INFO:data.data_loader:                           Bellek: 0.01 MB\n",
      "INFO:data.data_loader:departments              :         21 satır x   2 sütun\n",
      "INFO:data.data_loader:                           Bellek: 0.00 MB\n",
      "INFO:data.data_loader:================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham Veri Yüklendi.\n",
      "\n",
      "Özellik Üretim Hattı Başlatılıyor...\n",
      "1. Kesme Stratejisi Uygulanıyor (Geçmiş ve Gelecek Ayrımı Yapılıyor)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:features.rfm_features:RFM özellikleri oluşturuluyor...\n",
      "INFO:features.rfm_features:Yenilik özellikleri oluşturuluyor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Hedefler Oluşturuluyor...\n",
      "3. RFM Özellikleri Oluşturuluyor (Sadece Ham Metrikler)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:features.rfm_features:Sıklık özellikleri oluşturuluyor...\n",
      "INFO:features.rfm_features:Parasal özellikler oluşturuluyor (sepet büyüklüğünü vekil olarak kullanarak)...\n",
      "INFO:features.rfm_features:14 adet RFM özelliği oluşturuldu\n",
      "INFO:features.rfm_features:Özellikler: ['days_since_last_order', 'days_since_first_order', 'customer_age_days', 'avg_days_between_orders', 'total_orders', 'orders_per_day', 'order_regularity', 'std_days_between_orders', 'avg_basket_size', 'total_items_ordered', 'basket_size_std', 'basket_size_cv', 'avg_unique_products_per_order', 'total_unique_products_ordered']\n",
      "INFO:features.behavioral_features:Davranışsal özellikler oluşturuluyor...\n",
      "INFO:features.behavioral_features:Zaman bazlı özellikler oluşturuluyor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Davranışsal Özellikler Oluşturuluyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:104: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  weekend_orders = orders_df.groupby('user_id').apply(\n",
      "d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:110: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  night_orders = orders_df.groupby('user_id').apply(\n",
      "d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:116: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  morning_orders = orders_df.groupby('user_id').apply(\n",
      "d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:122: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  afternoon_orders = orders_df.groupby('user_id').apply(\n",
      "INFO:features.behavioral_features:Tekrar sipariş davranışı özellikleri oluşturuluyor...\n",
      "INFO:features.behavioral_features:Çeşitlilik özellikleri oluşturuluyor...\n",
      "d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:258: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  exploration_df = order_products_full.groupby('user_id').apply(calculate_exploration).reset_index(name='exploration_rate')\n",
      "INFO:features.behavioral_features:22 adet davranışsal özellik oluşturuldu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Zaman Serisi Eğilimleri Türetiliyor...\n",
      "6. Hız (Velocity) ve İvme (Acceleration) Metrikleri Oluşturuluyor...\n",
      "7. Özellik Kümeleri Birleştiriliyor...\n",
      "\n",
      "Üretim Hattı Tamamlandı. Veri Kümesi Boyutu (Shape): (206209, 46)\n",
      "\n",
      "Eğitim İçin Veri Hazırlanıyor...\n",
      "Veri bölme tamamlandı. X_train boyutu: (164967, 44), X_test boyutu: (41242, 44)\n",
      "En İyi Hiperparametreler önceki adımdan yüklendi.\n",
      "\n",
      "Nihai LightGBM Modeli Eğitiliyor...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.764151\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.764151\n",
      "\n",
      "Optimal Eşik Değeri Hesaplanıyor...\n",
      "Bulunan En İyi Eşik Değeri: 0.4565\n",
      "Eşik Değeri şuraya kaydedildi: d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\models\\optimal_threshold.json\n",
      "Model Eğitimi Tamamlandı.\n",
      "\n",
      "Model Performansı Doğrulanıyor...\n",
      "Nihai AUC Skoru: 0.7642\n",
      "Nihai F1 Skoru : 0.5893 (eşik değerinde 0.4565)\n",
      "\n",
      "Sınıflandırma Raporu (Classification Report):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.60      0.71     28605\n",
      "           1       0.47      0.79      0.59     12637\n",
      "\n",
      "    accuracy                           0.66     41242\n",
      "   macro avg       0.67      0.70      0.65     41242\n",
      "weighted avg       0.75      0.66      0.67     41242\n",
      "\n",
      "\n",
      "Üretim Yapıtları Dışa Aktarılıyor...\n",
      " -> Model şuraya kaydedildi: d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\models\\final_model_optimized.pkl\n",
      " -> Özellik listesi şuraya kaydedildi: d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\data\\processed\\model_features.json\n",
      " -> Özellik listesi şuraya senkronize edildi: d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\models\\feature_names.json\n",
      " -> İşlenmiş veri şuraya kaydedildi: d:\\egitim_ve_calismalar\\Lodos Makine Öğrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\data\\processed\\final_features_advanced.parquet\n",
      "\n",
      "ÜRETİM HATTI BAŞARIYLA TAMAMLANDI!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Veriyi Yükle\n",
    "    data_loader = InstacartDataLoader(RAW_DATA_DIR) \n",
    "    data_dict = data_loader.load_all_data()\n",
    "    print(\"Ham Veri Yüklendi.\")\n",
    "    \n",
    "    orders_df = data_dict['orders']\n",
    "    products_df = data_dict['products']\n",
    "\n",
    "    # Önceki ve eğitim siparişlerini birleştir\n",
    "    order_products = pd.concat([\n",
    "        data_dict['order_products_prior'],\n",
    "        data_dict['order_products_train']\n",
    "    ])\n",
    "    \n",
    "    # 2. Özellik Üretim Hattını Çalıştır\n",
    "    final_dataset = run_feature_pipeline(orders_df, order_products, products_df)\n",
    "    print(f\"\\nÜretim Hattı Tamamlandı. Veri Kümesi Boyutu (Shape): {final_dataset.shape}\")\n",
    "    \n",
    "    # 3. Modeli Eğit (Ayrıca Eşik Değerini de alıyoruz)\n",
    "    final_model, X_test, y_test, feature_cols, best_threshold = train_model(final_dataset)\n",
    "    \n",
    "    # 4. Doğrula (Eşik Değerini sağlıyoruz)\n",
    "    validate_model(final_model, X_test, y_test, best_threshold)\n",
    "    \n",
    "    # 5. Kaydet\n",
    "    save_artifacts(final_model, feature_cols, final_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
