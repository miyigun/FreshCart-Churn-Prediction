{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a756b2",
   "metadata": {},
   "source": [
    "<h2>06. Nihai Ãœretim HattÄ± (Final Production Pipeline)</h2>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>Proje DetaylarÄ±</h3>\n",
    "<ul>\n",
    "    <li><b>Proje:</b> FreshCart MÃ¼ÅŸteri KaybÄ± Tahmini (Customer Churn Prediction)</li>\n",
    "    <li><b>AmaÃ§:</b> UÃ§tan Uca Veri Ä°ÅŸleme ve Model EÄŸitimi HattÄ± (End-to-End Data Processing & Model Training Pipeline)</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3>AmaÃ§</h3>\n",
    "<p>\n",
    "    Bu betik, Ã¶nceki tÃ¼m adÄ±mlarÄ± (Veri YÃ¼kleme, Ã–zellik MÃ¼hendisliÄŸi ve Modelleme) tek, tekrarlanabilir bir <b>Ã¼retim hattÄ±nda</b> (pipeline) birleÅŸtirir. Bir Ã¼retim eÄŸitimi Ã§alÄ±ÅŸtÄ±rmasÄ±nÄ± simÃ¼le eder:\n",
    "</p>\n",
    "\n",
    "<ol>\n",
    "    <li><b>Ham Veriyi YÃ¼kle</b> (Load Raw Data)</li>\n",
    "    <li><b>Kesme Stratejisi Uygula</b> (SÄ±zÄ±ntÄ±yÄ± Ã–nle) (Apply Cutoff Strategy - Prevent Leakage)</li>\n",
    "    <li><b>TÃ¼m Ã–zellikleri OluÅŸtur</b> (RFM + DavranÄ±ÅŸsal + GeliÅŸmiÅŸ) (Generate All Features)</li>\n",
    "    <li><b>Nihai Modeli EÄŸit</b> (Optimize EdilmiÅŸ Hiperparametreleri Kullanarak) (Train Final Model)</li>\n",
    "    <li><b>DaÄŸÄ±tÄ±m Ä°Ã§in YapÄ±tlarÄ± DÄ±ÅŸa Aktar</b> (Model ve Meta Veriler) (Export Artifacts)</li>\n",
    "</ol>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KÃ¼tÃ¼phaneleri import etme\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, precision_recall_curve\n",
    "from scipy import stats\n",
    "\n",
    "# Proje yapÄ±landÄ±rmasÄ±\n",
    "from config import RAW_DATA_DIR, PROCESSED_DATA_DIR, MODEL_DIR, RANDOM_STATE, CHURN_DEFINITION\n",
    "\n",
    "# Veri yÃ¼kleyici\n",
    "from data.data_loader import InstacartDataLoader\n",
    "\n",
    "# Ã–zellik mÃ¼hendisliÄŸi sÄ±nÄ±flarÄ±\n",
    "sys.path.append('../src')\n",
    "from features.rfm_features import RFMFeatureEngineer\n",
    "from features.behavioral_features import BehavioralFeatureEngineer\n",
    "\n",
    "print(\"KÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a346f2",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "AdÄ±m 1: Ham Veriyi Al\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data():\n",
    "        \"\"\"Ham Instacart veri kÃ¼melerini yÃ¼kler ve Ã¶nceki/eÄŸitim sipariÅŸlerini birleÅŸtirir.\"\"\"\n",
    "                \n",
    "        print(\"Ham Veri YÃ¼kleniyor...\")\n",
    "        \n",
    "        # config.py dosyasÄ±ndaki yolu kullanarak veri yÃ¼kleyiciyi baÅŸlat\n",
    "        loader = InstacartDataLoader(RAW_DATA_DIR)\n",
    "        \n",
    "        # TÃ¼m veri kÃ¼melerini bir sÃ¶zlÃ¼ÄŸe (dictionary) yÃ¼kle\n",
    "        data = loader.load_all_data()\n",
    "\n",
    "        # Ana veri Ã§erÃ§evelerini (dataframes) ayÄ±r\n",
    "        orders_df = data['orders']\n",
    "        products_df = data['products']\n",
    "        aisles_df = data['aisles']\n",
    "        departments_df = data['departments']\n",
    "        \n",
    "        # Ã–nceki (prior) ve eÄŸitim (train) sipariÅŸ Ã¼rÃ¼nlerini tek bir veri Ã§erÃ§evesinde birleÅŸtir (concatenate)\n",
    "        order_products = pd.concat([\n",
    "            data['order_products_prior'],\n",
    "            data['order_products_train']\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        print(f\"Veri YÃ¼klendi. SipariÅŸler: {len(orders_df):,}, ÃœrÃ¼nler: {len(products_df):,}\")\n",
    "        \n",
    "        # Ãœretim hattÄ± (pipeline) iÃ§in gerekli tÃ¼m veri Ã§erÃ§evelerini dÃ¶ndÃ¼r\n",
    "        return orders_df, products_df, order_products, aisles_df, departments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34cfd7",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "AdÄ±m 2: Ã–zellik MÃ¼hendisliÄŸi HattÄ± (\"SÄ±zÄ±ntÄ±sÄ±z\" MantÄ±k)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trend(series):\n",
    "    \"\"\"\n",
    "    Zaman serisi eÄŸilimini (slope) hesaplar.\n",
    "    Hesaplama maliyetinden tasarruf etmek iÃ§in sadece en az 2 veri noktasÄ±na sahip seriler iÃ§in eÄŸimi (slope) hesaplar.\n",
    "    \"\"\"\n",
    "    if len(series) < 2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        # x zamandÄ±r (sipariÅŸ sÄ±rasÄ±), y deÄŸerdir (sepet bÃ¼yÃ¼klÃ¼ÄŸÃ¼ vb.)\n",
    "        # stats.linregress, doÄŸrusal regresyon parametrelerini dÃ¶ndÃ¼rÃ¼r.\n",
    "        slope, _, _, _, _ = stats.linregress(np.arange(len(series)), series.values)\n",
    "        return float(slope)\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_pipeline(orders_df, order_products, products_df, aisles_df, departments_df):\n",
    "    \"\"\"\n",
    "    03_feature_engineering.ipynb ile tamamen aynÄ± mantÄ±kta, veri sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nleyerek tÃ¼m Ã¶zellikleri Ã¼retir.\n",
    "    \"\"\"\n",
    "    print(\"\\nÃ–zellik Ãœretim HattÄ± BaÅŸlatÄ±lÄ±yor...\")\n",
    "    \n",
    "    # 1. GeÃ§miÅŸ ve gelecek ayrÄ±mÄ± (leakage Ã¶nleme)\n",
    "    print(\"1. Kesme Stratejisi UygulanÄ±yor (GeÃ§miÅŸ ve Gelecek AyrÄ±mÄ± YapÄ±lÄ±yor)...\")\n",
    "    orders_sorted = orders_df.sort_values(['user_id', 'order_number'])\n",
    "    last_orders = orders_sorted.groupby('user_id').tail(1)  # Hedef (Target)\n",
    "    orders_history = orders_sorted.drop(last_orders.index)  # Ã–zellikler GeÃ§miÅŸi (Features History)\n",
    "    \n",
    "    # order_products verilerini sadece geÃ§miÅŸ sipariÅŸler iÃ§in filtrele\n",
    "    op_history = order_products[order_products['order_id'].isin(orders_history['order_id'])]\n",
    "    \n",
    "    # 2. Churn etiketi oluÅŸtur\n",
    "    print(\"2. Churn Etiketleri OluÅŸturuluyor...\")\n",
    "    labels = last_orders[['user_id', 'days_since_prior_order']].copy()\n",
    "    # EÄŸer son sipariÅŸten bu yana geÃ§en gÃ¼n sayÄ±sÄ± >= 30 ise 'churn' (kayÄ±p) olarak etiketle\n",
    "    labels['is_churn'] = (labels['days_since_prior_order'] >= CHURN_DEFINITION).astype(int)\n",
    "    labels = labels[['user_id', 'is_churn']]\n",
    "\n",
    "    # 3. RFM Ã–zellikleri\n",
    "    print(\"3. RFM Ã–zellikleri OluÅŸturuluyor...\")\n",
    "    rfm_engineer = RFMFeatureEngineer(orders_history, op_history)\n",
    "    rfm_features = rfm_engineer.create_features()\n",
    "        \n",
    "    # 4. DavranÄ±ÅŸsal Ã–zellikler\n",
    "    print(\"4. DavranÄ±ÅŸsal Ã–zellikler OluÅŸturuluyor...\")\n",
    "    order_products_full = (\n",
    "        op_history\n",
    "        .merge(products_df, on='product_id', how='left')\n",
    "        .merge(aisles_df, on='aisle_id', how='left')\n",
    "        .merge(departments_df, on='department_id', how='left')\n",
    "    )\n",
    "    behavioral_engineer = BehavioralFeatureEngineer(orders_history, order_products_full)\n",
    "    behavioral_features = behavioral_engineer.create_features()\n",
    "    \n",
    "    # 5. Zaman Serisi EÄŸilimleri\n",
    "    print(\"5. Zaman Serisi EÄŸilimleri HesaplanÄ±yor...\")\n",
    "    # Sepet bÃ¼yÃ¼klÃ¼ÄŸÃ¼ trendi\n",
    "    basket_sizes = op_history.groupby(['user_id', 'order_id']).size().reset_index(name='basket_size')\n",
    "    basket_sizes = basket_sizes.merge(orders_history[['user_id', 'order_id', 'order_number']], \n",
    "                                    on=['user_id', 'order_id'])\n",
    "    basket_trend = (\n",
    "        basket_sizes.sort_values(['user_id', 'order_number'])\n",
    "        .groupby('user_id')['basket_size']\n",
    "        .apply(calculate_trend)\n",
    "        .reset_index(name='basket_size_trend')\n",
    "    )\n",
    "\n",
    "    # SipariÅŸ sÄ±klÄ±ÄŸÄ± ve yenilik ivmesi\n",
    "    order_freq_trend = (\n",
    "        orders_history.groupby('user_id')['days_since_prior_order']\n",
    "        .apply(calculate_trend)\n",
    "        .reset_index(name='order_frequency_trend')\n",
    "    )\n",
    "    recency_accel = (\n",
    "        orders_history.groupby('user_id')['days_since_prior_order']\n",
    "        .apply(calculate_trend)\n",
    "        .reset_index(name='recency_acceleration')\n",
    "    )\n",
    "    \n",
    "    # 6. SatÄ±n alma hÄ±zÄ± (velocity)\n",
    "    print(\"6. HÄ±z ve Ä°vme Metrikleri OluÅŸturuluyor...\")\n",
    "    user_stats = orders_history.groupby('user_id').agg(\n",
    "        total_orders=('order_id', 'count'),\n",
    "        total_days=('days_since_prior_order', 'sum')\n",
    "    ).reset_index()\n",
    "    user_stats['purchase_velocity'] = user_stats['total_orders'] / (user_stats['total_days'] + 1)\n",
    "\n",
    "    # 7. TÃ¼m Ã¶zellikleri birleÅŸtir\n",
    "    print(\"7. Ã–zellikler BirleÅŸtiriliyor...\")\n",
    "    final_features = (\n",
    "        labels\n",
    "        .merge(rfm_features, on='user_id', how='left')\n",
    "        .merge(behavioral_features, on='user_id', how='left')\n",
    "        .merge(basket_trend, on='user_id', how='left')\n",
    "        .merge(order_freq_trend, on='user_id', how='left')\n",
    "        .merge(recency_accel, on='user_id', how='left')\n",
    "        .merge(user_stats[['user_id', 'purchase_velocity']], on='user_id', how='left')\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    print(f\"Pipeline TamamlandÄ± â†’ Dataset Shape: {final_features.shape}\")\n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141bcd7",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "AdÄ±m 3: Nihai Model EÄŸitimi (Final Model Training)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset):\n",
    "    \"\"\"Optimize edilmiÅŸ hiperparametrelerle LightGBM modeli eÄŸitir.\"\"\"\n",
    "    print(\"\\nModel EÄŸitimi BaÅŸlatÄ±lÄ±yor...\")\n",
    "    feature_cols = [col for col in dataset.columns if col not in ['user_id', 'is_churn']]\n",
    "    X = dataset[feature_cols]\n",
    "    y = dataset['is_churn']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    print(f\"Veri bÃ¶lÃ¼ndÃ¼ â†’ X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "    # 04_model_optimization.ipynb'den gelen en iyi parametreler (Ã¶rnek)\n",
    "    best_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': 6,\n",
    "        'min_child_samples': 20,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "\n",
    "    # Optimal eÅŸik deÄŸeri (F1 maksimizasyonu)\n",
    "    print(\"Optimal EÅŸik DeÄŸeri HesaplanÄ±yor...\")\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "    f1_scores = 2 * recall * precision / (recall + precision + 1e-10)\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    print(f\"En Ä°yi EÅŸik DeÄŸeri: {best_threshold:.4f}\")\n",
    "\n",
    "    # EÅŸik deÄŸerini kaydet\n",
    "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MODEL_DIR / 'optimal_threshold.json', 'w') as f:\n",
    "        json.dump({'optimal_threshold': float(best_threshold)}, f)\n",
    "\n",
    "    return model, X_test, y_test, feature_cols, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729a98e",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "AdÄ±m 4: HÄ±zlÄ± DoÄŸrulama ve SaÄŸlama KontrolÃ¼ (Quick Validation & Sanity Check)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176015a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, X_test, y_test, threshold):\n",
    "    \"\"\"Model performansÄ±nÄ± raporlar.\"\"\"\n",
    "    print(\"\\nModel DoÄŸrulanÄ±yor...\")\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Final AUC: {auc:.4f}\")\n",
    "    print(f\"Final F1 (threshold {threshold:.4f}): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec16c08",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "AdÄ±m 5: DaÄŸÄ±tÄ±m Ä°Ã§in ÃœrÃ¼nleri DÄ±ÅŸa Aktar (Export Artifacts for Deployment)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artifacts(model, feature_cols, final_dataset):\n",
    "    \"\"\"Model ve meta verileri Ã¼retim iÃ§in kaydeder.\"\"\"\n",
    "    print(\"\\nÃœretim YapÄ±tlarÄ± Kaydediliyor...\")\n",
    "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    joblib.dump(model, MODEL_DIR / 'final_model_optimized.pkl')\n",
    "    with open(MODEL_DIR / 'feature_names.json', 'w') as f:\n",
    "        json.dump(feature_cols, f)\n",
    "    with open(PROCESSED_DATA_DIR / 'model_features.json', 'w') as f:\n",
    "        json.dump(feature_cols, f)\n",
    "\n",
    "    final_dataset.to_parquet(PROCESSED_DATA_DIR / 'final_features_advanced.parquet', index=False)\n",
    "\n",
    "    print(\"TÃ¼m yapÄ±tlar baÅŸarÄ±yla kaydedildi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f00a9",
   "metadata": {},
   "source": [
    "<h4><b>\n",
    "ANA Ã‡ALIÅTIRMA (MAIN EXECUTION)\n",
    "<h4><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49cf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data.data_loader:ğŸ“¦ Loading Instacart datasets...\n",
      "INFO:data.data_loader:   Loading orders.csv...\n",
      "INFO:data.data_loader:   âœ… Loaded orders: (3421083, 7)\n",
      "INFO:data.data_loader:   Loading order_products__prior.csv...\n",
      "INFO:data.data_loader:   âœ… Loaded order_products_prior: (32434489, 4)\n",
      "INFO:data.data_loader:   Loading order_products__train.csv...\n",
      "INFO:data.data_loader:   âœ… Loaded order_products_train: (1384617, 4)\n",
      "INFO:data.data_loader:   Loading products.csv...\n",
      "INFO:data.data_loader:   âœ… Loaded products: (49688, 4)\n",
      "INFO:data.data_loader:   Loading aisles.csv...\n",
      "INFO:data.data_loader:   âœ… Loaded aisles: (134, 2)\n",
      "INFO:data.data_loader:   Loading departments.csv...\n",
      "INFO:data.data_loader:   âœ… Loaded departments: (21, 2)\n",
      "INFO:data.data_loader:âœ… All datasets loaded successfully!\n",
      "\n",
      "INFO:data.data_loader:================================================================================\n",
      "INFO:data.data_loader:DATA SUMMARY\n",
      "INFO:data.data_loader:================================================================================\n",
      "INFO:data.data_loader:orders                   :  3,421,083 rows x   7 columns\n",
      "INFO:data.data_loader:                           Memory: 358.81 MB\n",
      "INFO:data.data_loader:order_products_prior     : 32,434,489 rows x   4 columns\n",
      "INFO:data.data_loader:                           Memory: 989.82 MB\n",
      "INFO:data.data_loader:order_products_train     :  1,384,617 rows x   4 columns\n",
      "INFO:data.data_loader:                           Memory: 42.26 MB\n",
      "INFO:data.data_loader:products                 :     49,688 rows x   4 columns\n",
      "INFO:data.data_loader:                           Memory: 5.31 MB\n",
      "INFO:data.data_loader:aisles                   :        134 rows x   2 columns\n",
      "INFO:data.data_loader:                           Memory: 0.01 MB\n",
      "INFO:data.data_loader:departments              :         21 rows x   2 columns\n",
      "INFO:data.data_loader:                           Memory: 0.00 MB\n",
      "INFO:data.data_loader:================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Raw Data Loaded.\n",
      "\n",
      "âš™ï¸ Starting Feature Pipeline...\n",
      "   1. Applying Cutoff Strategy (Splitting History vs Future)...\n",
      "   2. Generating Targets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:features.rfm_features:ğŸ”§ Creating RFM features...\n",
      "INFO:features.rfm_features:   Creating recency features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3. Creating RFM features (Raw Metrics Only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:features.rfm_features:   Creating frequency features...\n",
      "INFO:features.rfm_features:   Creating monetary features (using basket size as a proxy)...\n",
      "INFO:features.rfm_features:âœ… Created 14 RFM features\n",
      "INFO:features.rfm_features:   Features: ['days_since_last_order', 'days_since_first_order', 'customer_age_days', 'avg_days_between_orders', 'total_orders', 'orders_per_day', 'order_regularity', 'std_days_between_orders', 'avg_basket_size', 'total_items_ordered', 'basket_size_std', 'basket_size_cv', 'avg_unique_products_per_order', 'total_unique_products_ordered']\n",
      "INFO:features.behavioral_features:ğŸ§  Creating behavioral features...\n",
      "INFO:features.behavioral_features:   Creating time-based features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4. Generating Behavioral Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:104: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  weekend_orders = orders_df.groupby('user_id').apply(\n",
      "d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:110: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  night_orders = orders_df.groupby('user_id').apply(\n",
      "d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:116: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  morning_orders = orders_df.groupby('user_id').apply(\n",
      "d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:122: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  afternoon_orders = orders_df.groupby('user_id').apply(\n",
      "INFO:features.behavioral_features:   Creating reorder behavior features...\n",
      "INFO:features.behavioral_features:   Creating diversity features...\n",
      "d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\../src\\features\\behavioral_features.py:258: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  exploration_df = order_products_full.groupby('user_id').apply(calculate_exploration).reset_index(name='exploration_rate')\n",
      "INFO:features.behavioral_features:âœ… Created 22 behavioral features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5. Deriving Time-Series Trends (This takes time)...\n",
      "   6. Generating Velocity & Acceleration Metrics...\n",
      "   7. Merging Feature Sets...\n",
      "\n",
      "âœ… Pipeline Complete. Dataset Shape: (206209, 46)\n",
      "\n",
      "ğŸš€ Preparing Data for Training...\n",
      "âœ… Data split complete. X_train shape: (164967, 44), X_test shape: (41242, 44)\n",
      "âœ… Loaded Best Hyperparameters from previous step.\n",
      "\n",
      "ğŸš€ Training Final LightGBM Model...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.764437\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.764437\n",
      "\n",
      "âš–ï¸ Calculating Optimal Threshold...\n",
      "ğŸ† Best Threshold Found: 0.4272\n",
      "ğŸ’¾ Threshold saved to: d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\models\\optimal_threshold.json\n",
      "âœ… Model Training Complete.\n",
      "\n",
      "ğŸ“Š Validating Model Performance...\n",
      "Final AUC Score: 0.7644\n",
      "Final F1 Score : 0.5902 (at threshold 0.4272)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.59      0.70     28605\n",
      "           1       0.46      0.81      0.59     12637\n",
      "\n",
      "    accuracy                           0.66     41242\n",
      "   macro avg       0.67      0.70      0.65     41242\n",
      "weighted avg       0.75      0.66      0.67     41242\n",
      "\n",
      "\n",
      "ğŸ“¦ Exporting Production Artifacts...\n",
      "  -> Model saved to: d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\models\\final_model_optimized.pkl\n",
      "  -> Feature list saved to: d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\data\\processed\\model_features.json\n",
      "  -> Feature list synced to: d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\models\\feature_names.json\n",
      "  -> Processed data saved to: d:\\egitim_ve_calismalar\\Lodos Makine Ã–ÄŸrenmesi Bootcamp 02.11.2025\\html\\FreshCart-Churn-Prediction\\notebooks\\..\\data\\processed\\final_features_advanced.parquet\n",
      "\n",
      "ğŸ‰ PIPELINE FINISHED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Veri yÃ¼kle\n",
    "    orders_df, products_df, order_products, aisles_df, departments_df = load_raw_data()\n",
    "\n",
    "    # 2. Ã–zellik mÃ¼hendisliÄŸi\n",
    "    final_dataset = run_feature_pipeline(orders_df, order_products, products_df, aisles_df, departments_df)\n",
    "\n",
    "    # 3. Model eÄŸitimi\n",
    "    model, X_test, y_test, feature_cols, best_threshold = train_model(final_dataset)\n",
    "\n",
    "    # 4. DoÄŸrulama\n",
    "    validate_model(model, X_test, y_test, best_threshold)\n",
    "\n",
    "    # 5. Kaydet\n",
    "    save_artifacts(model, feature_cols, final_dataset)\n",
    "\n",
    "    print(\"\\nPIPELINE TAMAMLANDI! Model Ã¼retime hazÄ±r.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
